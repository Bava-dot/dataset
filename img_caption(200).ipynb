{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "download dataset from https://www.kaggle.com/datasets/adityajn105/flickr8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input, Dropout, add\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "DATASET_SIZE = 200\n",
    "images_dir = r'path to above downloaded image path'\n",
    "captions_file = r'path to above downloaded caption.txt file path'\n",
    "\n",
    "# 1. Load captions from captions.txt\n",
    "def load_captions(captions_file):\n",
    "    data = pd.read_csv(captions_file)\n",
    "    captions = {}\n",
    "    for _, row in data.iterrows():\n",
    "        image, caption = row['image'], f\"startseq {row['caption']} endseq\"\n",
    "        if image not in captions:\n",
    "            captions[image] = []\n",
    "        captions[image].append(caption)\n",
    "    return captions\n",
    "\n",
    "# 2. Extract features without a progress bar\n",
    "def extract_features(images_dir, limit=None):\n",
    "    model = InceptionV3(weights='imagenet')\n",
    "    model = Model(model.input, model.layers[-2].output)\n",
    "    features = {}\n",
    "    img_list = os.listdir(images_dir)[:limit] if limit else os.listdir(images_dir)\n",
    "    \n",
    "    for img_name in img_list:\n",
    "        img_path = os.path.join(images_dir, img_name)\n",
    "        try:\n",
    "            img = tf.keras.preprocessing.image.load_img(img_path, target_size=(299, 299))\n",
    "            img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "            img = np.expand_dims(img, axis=0)\n",
    "            img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "            tf.get_logger().setLevel('ERROR')\n",
    "            features[img_name] = model.predict(img).flatten()\n",
    "            tf.get_logger().setLevel('INFO')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {img_name}: {e}\")\n",
    "    return features\n",
    "\n",
    "# 3. Preprocess captions\n",
    "def preprocess_captions(captions):\n",
    "    all_captions = [caption for cap_list in captions.values() for caption in cap_list]\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(all_captions)\n",
    "    sequences = {img: tokenizer.texts_to_sequences(caps) for img, caps in captions.items()}\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    max_length = max(len(seq) for seq_list in sequences.values() for seq in seq_list)\n",
    "    return sequences, tokenizer, vocab_size, max_length\n",
    "\n",
    "# 4. Create training data\n",
    "def create_training_data(features, sequences, max_length, vocab_size, limit):\n",
    "    X1, X2, y = [], [], []\n",
    "    count = 0\n",
    "    for img, seq_list in sequences.items():\n",
    "        for seq in seq_list:\n",
    "            for i in range(1, len(seq)):\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                out_seq = tf.keras.utils.to_categorical(out_seq, num_classes=vocab_size)\n",
    "                X1.append(features[img])\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "                count += 1\n",
    "                if count >= limit:\n",
    "                    return np.array(X1), np.array(X2), np.array(y)\n",
    "    return np.array(X1), np.array(X2), np.array(y)\n",
    "\n",
    "# 5. Define CNN-LSTM model\n",
    "def define_model(vocab_size, max_length):\n",
    "    input_image = Input(shape=(2048,))\n",
    "    image_features = Dropout(0.5)(input_image)\n",
    "    image_features = Dense(256, activation='relu')(image_features)\n",
    "\n",
    "    input_sequence = Input(shape=(max_length,))\n",
    "    sequence_features = Embedding(vocab_size, 256, mask_zero=True)(input_sequence)\n",
    "    sequence_features = LSTM(256)(sequence_features)\n",
    "\n",
    "    decoder = add([image_features, sequence_features])\n",
    "    decoder = Dense(256, activation='relu')(decoder)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder)\n",
    "\n",
    "    model = Model(inputs=[input_image, input_sequence], outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    return model\n",
    "\n",
    "# 6. Generate caption for an image\n",
    "def generate_caption(model, tokenizer, photo, max_length):\n",
    "    caption = 'startseq'\n",
    "    for _ in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([caption])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        prediction = np.argmax(model.predict([photo, sequence], verbose=0))\n",
    "        word = tokenizer.index_word.get(prediction)\n",
    "        if word is None or word == 'endseq':\n",
    "            break\n",
    "        caption += ' ' + word\n",
    "    return caption.replace('startseq', '').replace('endseq', '').strip()\n",
    "\n",
    "# 7. Calculate BLEU scores\n",
    "def calculate_bleu_scores(model, tokenizer, features, captions, max_length):\n",
    "    scores = []\n",
    "    smoothing_function = SmoothingFunction().method4\n",
    "\n",
    "    # Ensure only keys present in both features and captions are used\n",
    "    valid_keys = set(features.keys()).intersection(set(captions.keys()))\n",
    "\n",
    "    for img_name in valid_keys:\n",
    "        ground_truths = captions[img_name]\n",
    "        photo = features[img_name].reshape((1, 2048))\n",
    "        generated_caption = generate_caption(model, tokenizer, photo, max_length)\n",
    "\n",
    "        ground_truths_cleaned = [\n",
    "            gt.replace('startseq', '').replace('endseq', '').strip().split()\n",
    "            for gt in ground_truths\n",
    "        ]\n",
    "        generated_caption_cleaned = generated_caption.split()\n",
    "\n",
    "        bleu_score = sentence_bleu(\n",
    "            ground_truths_cleaned,\n",
    "            generated_caption_cleaned,\n",
    "            smoothing_function=smoothing_function\n",
    "        )\n",
    "        scores.append(bleu_score)\n",
    "\n",
    "    return scores\n",
    "\n",
    "# 8. Display images with captions\n",
    "def display_images_with_captions(images_dir, features, model, tokenizer, max_length):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, img_name in enumerate(list(features.keys())[:5]):\n",
    "        img_path = os.path.join(images_dir, img_name)\n",
    "        img = Image.open(img_path)\n",
    "        photo = features[img_name].reshape((1, 2048))\n",
    "        caption = generate_caption(model, tokenizer, photo, max_length)\n",
    "\n",
    "        plt.subplot(2, 3, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(caption, fontsize=12)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    captions = load_captions(captions_file)\n",
    "    features = extract_features(images_dir, limit=DATASET_SIZE)\n",
    "    sequences, tokenizer, vocab_size, max_length = preprocess_captions(captions)\n",
    "    train_features = {img: features[img] for img in features.keys()}\n",
    "    train_sequences = {img: sequences[img] for img in features.keys()}\n",
    "\n",
    "    X1, X2, y = create_training_data(train_features, train_sequences, max_length, vocab_size, limit=5000)\n",
    "\n",
    "    model = define_model(vocab_size, max_length)\n",
    "    model.fit([X1, X2], y, epochs=5, batch_size=64, verbose=1)\n",
    "\n",
    "    # Calculate BLEU scores\n",
    "    bleu_scores = calculate_bleu_scores(model, tokenizer, features, captions, max_length)\n",
    "    print(f\"Average BLEU score: {np.mean(bleu_scores)}\")\n",
    "    for i, score in enumerate(bleu_scores[:5]):\n",
    "        print(f\"BLEU score for image {i + 1}: {score}\")\n",
    "\n",
    "    # Display images with generated captions\n",
    "    display_images_with_captions(images_dir, features, model, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_single_image(model, tokenizer, max_length, image_path):\n",
    "    \"\"\"\n",
    "    Generate a caption for a single image.\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    try:\n",
    "        img = tf.keras.preprocessing.image.load_img(image_path, target_size=(299, 299))\n",
    "        img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "\n",
    "        # Load the InceptionV3 model to extract features\n",
    "        feature_extractor = InceptionV3(weights='imagenet')\n",
    "        feature_extractor = Model(feature_extractor.input, feature_extractor.layers[-2].output)\n",
    "\n",
    "        # Extract features\n",
    "        features = feature_extractor.predict(img).flatten().reshape((1, 2048))\n",
    "\n",
    "        # Generate a caption\n",
    "        caption = generate_caption(model, tokenizer, features, max_length)\n",
    "        return caption\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing image: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "image_path = r'path to train/test image'  # Replace with the path to your image\n",
    "caption = inference_single_image(model, tokenizer, max_length, image_path)\n",
    "if caption:\n",
    "    print(f\"Generated Caption: {caption}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
